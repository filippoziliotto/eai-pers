# -------------------------------------------------------------------
# default.yaml
# Project-wide default configuration values
# -------------------------------------------------------------------

# -------------------------------------------------------------------
# DATA CONFIGURATION
# -------------------------------------------------------------------
data:
  data_dir: "data"                  # Path to dataset root directory
  data_split: "object_unseen"      # Which data split to use (e.g., seen, unseen)

# Random seed for reproducibility
seed: 2025


# -------------------------------------------------------------------
# AUGMENTATION SETTINGS
# Enables or disables different types of input augmentation
# -------------------------------------------------------------------
augmentations:
  use_aug: true                    # Global switch to enable augmentations
  default_prob: 0.5               # Default probability applied across augmentations

  flip:
    use_horizontal_flip: true     # Enable horizontal flip
    use_vertical_flip: true       # Enable vertical flip
    prob: 0.5                      # Probability of applying flip

  crop:
    use_crop: true                # Enable random cropping
    max_crop_fraction: 0.3        # Maximum allowed crop size (fraction of image)
    prob: 0.5                      # Probability of applying crop

  rotation:
    use_rotation: true            # Enable image rotation
    angle_range: 90               # Maximum angle (degrees) for random rotation
    prob: 0.5                      # Probability of applying rotation

  desc:
    use_desc: true                # Enable description/textual augmentation
    prob: 0.5                      # Probability of applying description augmentation


# -------------------------------------------------------------------
# MODEL & TRAINING CONFIGURATION
# Model architecture, loss, training schedule, etc.
# -------------------------------------------------------------------
training:
  batch_size: 4                    # Number of samples per training batch
  validate_after_n_epochs: 1      # Frequency of validation (in epochs)

  loss:
    choice: "L2"                   # Loss type: L2, L1, etc.
    scaling: 0.3                   # Loss scaling factor


# -------------------------------------------------------------------
# MODEL ARCHITECTURE DETAILS
# Includes attention mechanism, encoder freezing, and model head
# -------------------------------------------------------------------
attention:
  embed_dim: 512                   # Dimension of attention embeddings
  num_heads: 8                     # Number of attention heads

model:
  type: "base"                     # Model architecture variant
  tau: 0.8                         # Softmax/contrastive temperature

encoder:
  freeze: true                     # Freeze encoder weights during training


# -------------------------------------------------------------------
# MAP MODULE CONFIGURATION
# Spatial representation parameters
# -------------------------------------------------------------------
map:
  size: 50                         # Spatial resolution (e.g., 50x50 grid)
  embedding_size: 768              # Size of spatial embedding vectors


# -------------------------------------------------------------------
# OPTIMIZATION PARAMETERS
# Optimizer choice and learning rate scheduling
# -------------------------------------------------------------------
optimizer:
  type: "adam"                     # Optimizer type
  lr: 0.001                        # Learning rate
  weight_decay: 1e-5               # Weight decay (L2 regularization)

scheduler:
  type: "none"                     # Learning rate scheduler (e.g., 'step', 'plateau')
  step_size: 5                     # Step size (used if type is 'step')
  gamma: 0.1                       # Decay factor for scheduler
  patience: 10                     # Patience for ReduceLROnPlateau


# -------------------------------------------------------------------
# CHECKPOINT MANAGEMENT
# Saving/loading model weights
# -------------------------------------------------------------------
checkpoint:
  save: false                      # Whether to save checkpoints
  path: "model/checkpoints/model.pth"  # Path to save/load model checkpoint
  load: false                      # Whether to load from checkpoint
  resume_training: false           # Resume whole training from checkpoint


# -------------------------------------------------------------------
# SYSTEM AND DEVICE CONFIGURATION
# -------------------------------------------------------------------
device:
  type: "mps"                      # Device type: 'cpu', 'cuda', 'mps', etc.
  num_workers: 4                   # Number of data loading workers


# -------------------------------------------------------------------
# LOGGING CONFIGURATION
# External logging (e.g., Weights & Biases)
# -------------------------------------------------------------------
logging:
  wandb:
    use_wandb: false               # Enable wandb logging
    run_name: "none"              # Name of the wandb run


# -------------------------------------------------------------------
# DEBUGGING & VISUALIZATION FLAGS
# Toggle visual/debugging features
# -------------------------------------------------------------------
debug: false                       # Enable debugging outputs
visualize: false                   # Enable visualizations
use_obstacle_map: false           # Use binary obstacle maps in planning


# -------------------------------------------------------------------
# BASELINE CONFIGURATION
# Toggle and specify baseline models for comparison
# -------------------------------------------------------------------
baseline:
  use_baseline: false             # Enable baseline comparisons
  type: "random"                  # Baseline policy type


# -------------------------------------------------------------------
# EXTERNAL FEATURE EXTRACTOR USAGE
# -------------------------------------------------------------------
use_extractor: true               # Use pretrained feature extractor (e.g., CLIP, RN50)
